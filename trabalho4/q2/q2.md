## Questão 2

Não foi encontrada a implementação dos métodos quadrados no AVA. Portanto, o grupo implementou sua própria versão.

### Criação dos dados

Assim ficou o script de criação dos dados:

```c
int main(int argc, char *argv[]) {
    FILE *f = fopen("dados.txt", "w");
    if (!f) {
        perror("Erro ao abrir arquivo");
        return 1;
    }

    int N = 100; // número de pontos padrão
    if (argc > 1) {
        N = atoi(argv[1]);
        if (N <= 0) {
            fprintf(stderr, "Valor de N inválido.\n");
            fclose(f);
            return 1;
        }
    }

    double a = 3.0, b = 2.5; // y = 3x + 2.5
    srand(time(NULL));

    fprintf(f, "%d\n", N);
    for (int i = 0; i < N; i++) {
        double x = i;
        double noise = ((rand() % 100) / 100.0 - 0.5); // erro entre -0.5 e +0.5
        double y = a * x + b + noise;
        fprintf(f, "%lf %lf\n", x, y);
    }

    fclose(f);
    return 0;
}
```

Cada linha do arquivo após a primeira equivale a um ponto no plano cartesiano. Foi aplicado um "barulho" de +-0.5 em cada ponto,
a partir de uma função linear arbitrária.

- **Volume dos dados**:
    - O arquivo com $10^8$ pontos chegou a ter 3.3GB.

### Mínimos quadrados via MPI_Bcast e via MPI_Isend/Irecv

Na primeira implementação, `minimos.c`, usamos `MPI_Bcast` para enviar os vetores de x e de y para cada processo:

```c
MPI_Bcast(x, N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
MPI_Bcast(y, N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
```

Na segunda implementação, `minimossend.c`, usamos `MPI_Isend` para enviar os vetores x e y para cada processo a partir do master,
e `MPI_Irecv` para recebê-los:

```c
if (rank == 0) {
    for (int dest = 1; dest < size; dest++) { // Thread master envia
        MPI_Isend(x, N, MPI_DOUBLE, dest, 0, MPI_COMM_WORLD, &requests[0]);
        MPI_Isend(y, N, MPI_DOUBLE, dest, 1, MPI_COMM_WORLD, &requests[1]);
        MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);
    }
} else { // Threads slave recebem
    MPI_Irecv(x, N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &requests[2]);
    MPI_Irecv(y, N, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD, &requests[3]);
    MPI_Waitall(2, &requests[2], MPI_STATUSES_IGNORE);
}
```

Em ambos os casos, recebemos a soma final por meio de um reduce nas somas parciais:

```c
MPI_Reduce(&local_sum_x, &sum_x, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
MPI_Reduce(&local_sum_y, &sum_y, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
MPI_Reduce(&local_sum_x2, &sum_x2, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
MPI_Reduce(&local_sum_xy, &sum_xy, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
```

### Resultados

Na hora de rodar o programa, nos deparamos com um problema: mesmo para o valor $N=10^8$, a parte paralela do programa
rodava muito rapidamente em comparação com a parte sequencial (que lê o arquivo e portanto é I/O). Por isso, não obtivemos
curvas de speedup interessantes para demonstrar, tendo ficado o exercício apenas de implementação das duas versões dos
programas.
